{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Models\n",
    "\n",
    "A matching model (an instance of class `MatchingModel`) is a neural network to perform entity matching. It takes in the contents of a tuple pair, i.e., two sequnces of words for each attribute, as input and produces a match score as output. This tutorial describes the structure of this network and presents options available for each of its components. \n",
    "\n",
    "Important Note: Be aware that creating a matching model (`MatchingModel`) object does not immediately instantiate all its components - `deepmatcher` uses a lazy initialization paradigm where components are instantiated just before training. Hence, code examples in this tutorial manually perform this initialization to demonstrate model customization meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 17757810 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "import deepmatcher as dm\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "logging.getLogger('deepmatcher.core').setLevel(logging.INFO)\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = dm.data.process(\n",
    "    path='sample_data/itunes-amazon',\n",
    "    train='train.csv',\n",
    "    validation='validation.csv',\n",
    "    test='test.csv',\n",
    "    ignore_columns=('left_id', 'right_id'))\n",
    "\n",
    "model = dm.MatchingModel()\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on lazy initialization, please refer to the [Lazy Initialization](#Design-Note:-Lazy-Initialization) section of this tutorial.\n",
    "\n",
    "At its core, a matching model has 3 main components: 1. Attribute Embedding, 2. Attribute Similarity Representation, and 3. Classifier. This is illustrated in the figure below:\n",
    "\n",
    "<img src=\"../docs/source/_static/arch.png\" alt=\"Matching model structure\" style=\"width: 75%; margin-right: 200px;\"/>\n",
    "\n",
    "We briefly describe these components below. For a more in-depth explanation, please take a look at [our paper](http://pages.cs.wisc.edu/~anhai/papers1/deepmatcher-sigmod18.pdf). \n",
    "\n",
    "The 3 components are further broken down into sub-modules as shown:\n",
    "\n",
    "1. [Attribute Embedding](#1.-Attribute-Embedding)\n",
    "2. [Attribute Similarity Representation](#2.-Attribute-Similarity-Representation)\n",
    "    1. [Attr Summarizer](#2.1.-Attribute-Summarization)\n",
    "        1. [Word Contextualizer](#2.1.1.-Word-Contextualizer)\n",
    "        2. [Word Comparator](#2.1.2.-Word-Comparator)\n",
    "        3. [Word Aggregator](#2.1.3.-Word-Aggregator)\n",
    "    2. [Attr Comparator](#2.2.-Attribute-Comparator)\n",
    "3. [Classifier](#3.-Classifier)\n",
    "\n",
    "## 1. Attribute Embedding\n",
    "\n",
    "The Attribute Embedding component (AE) takes in two sequences of words corresponding to the value of each attribute and converts each word in them to a word embedding (vector representation of a word). This produces two sequences of word embeddings as output for each attribute. This is illustrated in the figure below. For an intuitive explanation of word embeddings, please refer [this blog post](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/#word-embeddings). The Attribute Embedding component is also presented in more detail in [our talk](http://bit.do/deepmatcher-talk). Note that this component is shared across all attributes - the same AE model is used for all attributes.\n",
    "\n",
    "<img src=\"../docs/source/_static/ae.png\" alt=\"Attribute Embedding\" style=\"width: 25%;\"/>\n",
    "\n",
    "### Customizing Attribute Embedding\n",
    "\n",
    "This component uses word embeddings that were loaded as part of data processing. To customize it, you can set the `embeddings` parameter in `dm.data.process`, as described in the [tutorial on data processing](https://nbviewer.jupyter.org/github/sidharthms/deepmatcher/blob/master/examples/data_processing.ipynb#3.-Word-Embeddings).\n",
    "\n",
    "## 2. Attribute Similarity Representation\n",
    "\n",
    "This component (ASR) takes attribute value embeddings, i.e., two sequences of word embeddings, and encodes them into a representation that captures their similarities and differences. Its operations are split between two modules as described below and as illustrated in the following figure:\n",
    "\n",
    "**2.1 Attribute Summarization (AS):** This module takes as input the two word embedding sequences and summarizes the information in them to produce two summary vectors as output. The role of attribute summarization is to aggregate information across all tokens in an attribute value sequence of an entity mention. This summarization process may\n",
    "consider the pair of sequences of an attribute jointly to perform more sophisticated operations such as alignment. Folks in NLP: this has nothing to do with [text summarization](https://en.wikipedia.org/wiki/Automatic_summarization).\n",
    "\n",
    "**2.2 Attribute Comparison (AC):** This module takes as input the two summary vectors and applies a comparison function over those summaries to obtain the final similarity representation of the two attribute values. \n",
    "\n",
    "<img src=\"../docs/source/_static/asr.png\" alt=\"Matching model structure\" style=\"width: 50%;\"/>\n",
    "\n",
    "Note that unlike Attribute Embedding, this component is not shared across attributes - each attribute has its own dedicated ASR model. They share the same structure but do not share their parameters.\n",
    "\n",
    "### Customizing Attribute Similarity Representation\n",
    "\n",
    "The ASR can be customized by specifying the `attr_summarizer` and optionally the `attr_comparator` parameter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 662602 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_summarizer='sif', attr_comparator='diff')\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `attr_summarizer` can be set to one of the following:\n",
    "\n",
    "- A string: One of the following 4 string literals\n",
    "  1. 'sif': Use the SIF attribute summarizer (refer [our paper](http://pages.cs.wisc.edu/~anhai/papers1/deepmatcher-sigmod18.pdf) for details on SIF and other attribute summarizers). Equivalent to setting `attr_summarizer = dm.attr_summarizers.SIF()`.\n",
    "  2. 'rnn': Use the RNN attribute summarizer. Equivalent to setting `attr_summarizer = dm.attr_summarizers.RNN()`.\n",
    "  3. 'attention': Use the Attention attribute summarizer. Equivalent to setting `attr_summarizer = dm.attr_summarizers.Attention()`.\n",
    "  4. 'hybrid': Use the Hybrid attribute summarizer. Equivalent to setting `attr_summarizer = dm.attr_summarizers.Hybrid()`.\n",
    "\n",
    "\n",
    "- An instance of `dm.AttrSummarizer` or one of its subclasses:\n",
    "  1. An instance of [`dm.attr_summarizers.SIF`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/attr_summarizers.html#deepmatcher.models.attr_summarizers.SIF): Use the SIF attribute summarizer.\n",
    "  2. An instance of [`dm.attr_summarizers.RNN`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/attr_summarizers.html#deepmatcher.models.attr_summarizers.RNN): Use the RNN attribute summarizer.\n",
    "  3. An instance of [`dm.attr_summarizers.Attention`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/attr_summarizers.html#deepmatcher.models.attr_summarizers.Attention): Use the Attention attribute summarizer.\n",
    "  4. An instance of [`dm.attr_summarizers.Hybrid`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/attr_summarizers.html#deepmatcher.models.attr_summarizers.Hybrid): Use the Hybrid attribute summarizer.\n",
    "\n",
    "\n",
    "- A [`callable`](https://docs.python.org/3/library/functions.html#callable): Put simply, a function that returns a PyTorch [Module](http://pytorch.org/docs/master/nn.html#torch.nn.Module). The module must behave like an Attribute Summarizer, i.e., takes two word embedding sequences, summarizes the information in them and returns two vectors as output. Note that we cannot accept a PyTorch module directly as input because we may need to create multiple instances of this module, one for each attribute. Thus, we require that you specify custom modules via a `callable`.\n",
    "  - Input to module: Two 3d tensors of shape `(batch, seq1_len, input_size)` and  `(batch, seq2_len, input_size)`. These tensors will be wrapped within [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor)s which will contain metadata about the batch. \n",
    "  - Expected output from module: Two 2d tensors of shape `(batch, output_size)`, wrapped within [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor)s (with metadata information unchanged). `output_size` need not be the same as `input_size`.\n",
    "\n",
    "`string` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 662602 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_summarizer='sif')\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dm.AttrSummarizer` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 3917002 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_summarizer=dm.attr_summarizers.RNN())\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`callable` arg example: We create a custom attribute summarizer, one that simply sums up all the word embeddings in each sequence. To do this we use two helper modules:\n",
    "- [`dm.modules.Lambda`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Lambda): Used to create PyTorch module from a lambda function without having to define a class.\n",
    "- [`dm.modules.NoMeta`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.NoMeta): Used to remove metadata information from the input and restore it back in the output.\n",
    "\n",
    "Note that since we are using a custom `attr_summarizer`, the `attr_comparator` must be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 662602 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "my_attr_summarizer_module = dm.modules.NoMeta(dm.modules.Lambda(lambda x, y: (x.sum(dim=1), y.sum(dim=1))))\n",
    "\n",
    "model = dm.MatchingModel(attr_summarizer=lambda: my_attr_summarizer_module, attr_comparator='abs-diff')\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `attr_comparator` can be set to one of the following:\n",
    "\n",
    "- A string: One of the `style`s supported by the [`dm.modules.Merge`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Merge) module.\n",
    "- An instance of [`dm.modules.Merge`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Merge)\n",
    "- A [`callable`](https://docs.python.org/3/library/functions.html#callable): Put simply, a function that returns a PyTorch [Module](http://pytorch.org/docs/master/nn.html#torch.nn.Module). The module must take in two vectors as input and produces one vector as output.\n",
    "  - Input to module: Two 2d tensors of shape `(batch, input_size)`. \n",
    "  - Expected output from module: One 2d tensor of shape `(batch, output_size)`. `output_size` need not be the same as `input_size`.\n",
    "\n",
    "`string` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 17517810 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_comparator='concat')\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dm.modules.Merge` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 17277810 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_comparator=dm.modules.Merge('mul'))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`callable` arg example: We create a custom attribute comparator, one that concatenates the two attribute summaries and their element-wise product. We use the [`dm.modules.Lambda`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Lambda) helper module again to create PyTorch module from a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 17757810 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "my_attr_comparator_module = dm.modules.Lambda(lambda x, y: torch.cat((x, y, x * y), dim=x.dim() - 1))\n",
    "\n",
    "model = dm.MatchingModel(attr_comparator=lambda: my_attr_comparator_module)\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `attr_comparator` is not set, `deepmatcher` will try to automatically set it based on the `attr_summarizer` specified. The following mapping shows the `attr_comparator` that will be used for various kinds of atrribute summarizers: \n",
    "- Instance of `dm.attr_summarizers.SIF`: `attr_comparator='abs-diff'`\n",
    "- Instance of `dm.attr_summarizers.RNN`: : `attr_comparator='abs-diff'`\n",
    "- Instance of `dm.attr_summarizers.Attention`: : `attr_comparator='concat'`\n",
    "- Instance of `dm.attr_summarizers.Hybrid`: : `attr_comparator='concat-abs-diff'`\n",
    "\n",
    "If the specified `attr_summarizer` is not a supported string and is not an instance of any of these classes, then `attr_comparator` must be specified.\n",
    "\n",
    "## 2.1. Attribute Summarization\n",
    "\n",
    "The Attribute Summarization module is the most critical component of a matching model. As mentioned earlier, it takes in two word embedding sequences and summarizes the information in them to produce two summary vectors as output. It consists of 3 sub-modules, described below and illustrated in the following figure:\n",
    "\n",
    "<img src=\"../docs/source/_static/as.png\" alt=\"Attribute Summarization\" style=\"width: 100%; margin-top:30pt; margin-bottom:30pt;\"/>\n",
    "\n",
    "### 2.1.1. Word Contextualizer\n",
    "This is an optional module that takes as input a word embedding sequence and produces a *context-aware* word embedding sequence as output. For example, consider the raw word embedding sequence for sentences \"Brand : Orange\" and \"Color : Orange\". In the first case, the output word embedding for \"Orange\" may be adjusted to represent the color orange, and in the second case, it may be adjusted to represent the brand orange. This module is shared for both word embedding sequences, i.e., the same neural network is used for both sequences. \n",
    "\n",
    "### 2.1.2. Word Comparator\n",
    "This is an optional module takes as input two word embedding sequence (may or may not be context-aware), one of which is treated as the primary sequence and the other is treated as *context*. Intuitively, this modules does the following: \n",
    "- For each word in the primary sequence, find the corresponding aligning word in the context sequence.\n",
    "- Compare each word in the primary sequence with its corresponding word in the context sequence to obtain a *word comparison vector* for each word.\n",
    "\n",
    "The output of this module is the sequence of word comparison vectors, i.e., one word comparison vector for each word in the primary word embedding sequence. This module is shared for both word embedding sequences, i.e., the 1<sup>st</sup> word embedding sequence is compared to the 2<sup>nd</sup> to obtain a word comparison vector sequence for the 1<sup>st</sup> sequence, and the same network is used to compare the 2<sup>nd</sup> sequence to the 1<sup>st</sup> to obtain a word comparison vector sequence for the 2<sup>nd</sup> sequence.\n",
    "\n",
    "### 2.1.3. Word Aggregator\n",
    "This module takes as input a sequence of vectors - either a sequence of word embedding or a sequence of word comparison vectors. It aggregates this sequence to produce a single vector as summarizing this sequence. This module may optionally make use of the other sequence as context. This module is shared for both word embedding sequences, i.e., the same neural network is used for both sequences. \n",
    "\n",
    "\n",
    "### Customizing Attribute Summarization\n",
    "\n",
    "Attribute Summarization can be customized by specifying the `word_contextualizer`, `word_comparator`, and `word_aggregator` parameters while creating a `dm.AttrSummarizer` or any of its four sub-classes discussed above. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 11973802 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(\n",
    "    attr_summarizer=dm.attr_summarizers.Hybrid(\n",
    "        word_contextualizer='self-attention',\n",
    "        word_comparator='bilinear-attention',\n",
    "        word_aggregator='inv-freq-avg-pool'))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `word_contextualizer` can be set to one of the following:\n",
    "\n",
    "- A string: One of the `unit_type` supported by : [`dm.word_contextualizers.RNN`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/word_contextualizers.html#deepmatcher.models.word_contextualizers.RNN) or 'self-attention'\n",
    "  1. 'gru': Equivalent to setting `word_contextualizer = dm.word_contextualizers.RNN(unit_type='gru')`.\n",
    "  2. 'lstm': Equivalent to setting `word_contextualizer = dm.word_contextualizers.RNN(unit_type='lstm')`.\n",
    "  3. 'rnn': Equivalent to setting `word_contextualizer = dm.word_contextualizers.RNN(unit_type='rnn')`.\n",
    "  4. 'self-attention': Equivalent to setting `word_contextualizer = dm.word_contextualizers.SelfAttention()`.\n",
    "  \n",
    "  \n",
    "- An instance of `dm.WordContextualizer` or one of its subclasses:\n",
    "  1. An instance of [`dm.word_contextualizers.RNN`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/word_contextualizers.html#deepmatcher.models.word_contextualizers.RNN): Use the RNN word contextualizer.\n",
    "  2. An instance of [`dm.word_contextualizers.SelfAttention`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/word_contextualizers.html#deepmatcher.models.word_contextualizers.SelfAttention): Use the Self-Attention word contextualizer.\n",
    "  \n",
    "  \n",
    "- A [`callable`](https://docs.python.org/3/library/functions.html#callable): Put simply, a function that returns a PyTorch [Module](http://pytorch.org/docs/master/nn.html#torch.nn.Module).\n",
    "  - Input to module: One 3d tensor of shape `(batch, seq_len, input_size)`. The tensor will be wrapped within [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor) which will contain metadata about the batch.\n",
    "  - Expected output from module: One 3d tensor of shape`(batch, seq_len, output_size)`, wrapped within an [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor) (with metadata information unchanged). `output_size` need not be the same as `input_size`.\n",
    "\n",
    "We show some examples on how to customize word contextualizers for Hybrid attribute summarization modules (`dm.attr_summarizers.Hybrid`) below, but these are also applicable to other attribute summarizers:\n",
    "\n",
    "`string` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 20645010 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(\n",
    "    attr_summarizer=dm.attr_summarizers.Hybrid(word_contextualizer='gru'))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dm.WordContextualizer` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 26059410 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: dm.AttrSummarizer arg.\n",
    "model = dm.MatchingModel(\n",
    "    attr_summarizer = dm.attr_summarizers.Hybrid(\n",
    "        word_contextualizer=dm.word_contextualizers.SelfAttention(heads=2)))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`callable` arg example: We create a custom convolutional word contextualizer. To do this, we need the `input_size` dimension. This will be provided if the `callable` takes in one argument named `input_size` as shown. We then use this input size to create a convolutional layer. But the convolutional layer expects the sequence length dimension to be last. To deal with this we swap the 2nd and 3rd dimensions of the tensor before and after convolution. We also use [`dm.modules.Lambda`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Lambda) and [`dm.modules.NoMeta`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.NoMeta) as in earlier examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 38093682 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def my_word_contextualizer(input_size):  #  Function takes in 1 arg, so it'll be set to the `input_size`.\n",
    "    return dm.modules.NoMeta(torch.nn.Sequential(\n",
    "        dm.modules.Lambda(lambda x: x.transpose(1, 2)),  # Transpose the 2nd and 3rd dimensions for convolution.\n",
    "        torch.nn.Conv1d(in_channels=input_size, out_channels=512, kernel_size=3, padding=1),\n",
    "        dm.modules.Lambda(lambda x: x.transpose(1, 2))))  # Transpose the 2nd and 3rd dimensions back.\n",
    "\n",
    "model = dm.MatchingModel(attr_summarizer=dm.attr_summarizers.Hybrid(\n",
    "    word_contextualizer=my_word_contextualizer))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `word_comparator` can be set to one of the following:\n",
    "\n",
    "- A string: One of the following 3 string literals:\n",
    "  1. 'decomposable-attention': Equivalent to setting `word_comparator = dm.word_comparators.Attention(alignment_network='decomposable')`.\n",
    "  2. 'bilinear-attention': Equivalent to setting `word_comparator = dm.word_comparators.Attention(alignment_network='bilinear')`.\n",
    "  3. 'dot-attention': Equivalent to setting `word_comparator = dm.word_comparators.Attention(alignment_network='dot')`.\n",
    "  \n",
    "  \n",
    "- An instance of `dm.WordComparator` or one of its subclasses:\n",
    "  1. An instance of `dm.word_comparators.Attention`: Use the Attention word comparator.  \n",
    "  \n",
    "  \n",
    "- A [`callable`](https://docs.python.org/3/library/functions.html#callable): Put simply, a function that returns a PyTorch [Module](http://pytorch.org/docs/master/nn.html#torch.nn.Module). \n",
    "  - Inputs to module: Four input tensors, all of which will be wrapped within [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor)s.\n",
    "    1. Primary context-aware word embedding sequence. Shape: `(batch, seq1_len, input_size)`\n",
    "    2. Secondary context-aware word embedding sequence, i.e., the sequence to compare the primary sequence with. Shape: `(batch, seq1_len, input_size)`\n",
    "    3. Raw word embedding sequence (context-unaware). Shape: `(batch, seq1_len, raw_input_size)`\n",
    "    4. Raw secondary context-aware word embedding sequence (context-unaware). Shape: `(batch, seq2_len, raw_input_size)`\n",
    "  - Expected output from module: One 3d tensor of shape`(batch, seq1_len, output_size)`, wrapped within an [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor) (with the same metadata information as the first input tensor). `output_size` need not be the same as `input_size`.\n",
    "  - Notes:\n",
    "    - The custom module may choose to ignore the last two raw context-unaware inputs if they are deemed unnecessary.\n",
    "    - If no Word Contextualizer is used, the last two inputs will be the same as the first two inputs. \n",
    "\n",
    "We show some examples on how to customize word comparators for Hybrid attribute summarization modules (`dm.attr_summarizers.Hybrid`) below, but these are also applicable to other attribute summarizers:\n",
    "\n",
    "`string` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 20645010 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(\n",
    "    attr_summarizer=dm.attr_summarizers.Hybrid(word_comparator='dot-attention'))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dm.WordComparator` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 29675010 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(\n",
    "    attr_summarizer = dm.attr_summarizers.Hybrid(\n",
    "        word_comparator=dm.word_comparators.Attention(heads=4, input_dropout=0.2)))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`callable` arg example: We create a custom word comparator, one that uses the Attention word comparator but has a 2 layer RNN following it. Since there are multiple inputs, we cannot use the standard `torch.nn.Sequential`, but we can instead use the [`dm.modules.MultiSequential`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.MultiSequential) utility module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 27153810 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_summarizer=dm.attr_summarizers.Hybrid(\n",
    "    word_comparator=lambda: dm.modules.MultiSequential(\n",
    "        dm.word_comparators.Attention(),\n",
    "        dm.modules.RNN(unit_type='gru', layers=2))))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `word_aggregator` can be set to one of the following:\n",
    "\n",
    "- A string: One of the following string literals:\n",
    "  - One of the `style`s supported by the [`dm.modules.Pool`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Pool) module suffixed by '-pool', e.g., 'avg-pool', 'sif-pool', 'divsqrt-pool', etc.\n",
    "    - Equivalent to setting `word_aggregator = dm.word_aggregators.Pool(<pool_type>)`\n",
    "    - E.g. equivalent to setting `word_aggregator = dm.word_aggregators.Pool('avg')`\n",
    "  - 'attention-with-rnn': Equivalent to setting `word_aggregator = dm.word_aggregators.AttentionWithRNN()`\n",
    "  \n",
    "  \n",
    "- An instance of `dm.WordAggregator` or one of its subclasses:\n",
    "  1. An instance of [`dm.word_aggregators.Pool`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/word_aggregators.html#deepmatcher.models.word_aggregators.Pool): Use the Pool word aggregator.\n",
    "  2. An instance of [`dm.word_aggregators.AttentionWithRNN`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/word_aggregators.html#deepmatcher.models.word_aggregators.AttentionWithRNN): Use the AttentionWithRNN word aggregator.\n",
    "  \n",
    "  \n",
    "- A [`callable`](https://docs.python.org/3/library/functions.html#callable): Put simply, a function that returns a PyTorch [Module](http://pytorch.org/docs/master/nn.html#torch.nn.Module).\n",
    "  - Input to module: Two 3d tensor of shape `(batch, seq1_len, input_size)` and  `(batch, seq2_len, input_size)`. The tensors will be wrapped within [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor)s which will contain metadata about the batch.\n",
    "  - Expected output from module: One 2d tensor of shape`(batch, output_size)`, wrapped within an [`AttrTensor`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/batch.html#deepmatcher.batch.AttrTensor) (with the same metadata information as the first input tensor). This output must be the aggregation of the sequence of vectors in the first input  (primary input), optionally taking into account the context input, i.e., the second input. `output_size` need not be the same as `input_size`.\n",
    "\n",
    "We show some examples on how to customize word aggregators for Hybrid attribute summarization modules (`dm.attr_summarizers.Hybrid`) below, but these are also applicable to other attribute summarizers:\n",
    "\n",
    "`string` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 11616202 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(\n",
    "    attr_summarizer=dm.attr_summarizers.Hybrid(word_aggregator='max-pool'))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dm.WordAggregator` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 21729810 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(\n",
    "    attr_summarizer = dm.attr_summarizers.Hybrid(\n",
    "        word_aggregator=dm.word_aggregators.AttentionWithRNN(rnn='lstm', rnn_pool_style='max')))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`callable` arg example: We create a custom word aggregator, one that concatenates the average and max of the given input sequence. We also use [`dm.modules.Lambda`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Lambda) and [`dm.modules.NoMeta`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.NoMeta) as in earlier examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 11856202 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "my_word_aggregator_module = dm.modules.NoMeta(dm.modules.Lambda(\n",
    "    lambda x, y: torch.cat((x.mean(dim=1), x.max(dim=1)[0]), dim=-1)))\n",
    "\n",
    "# Next, create the matching model.\n",
    "model = dm.MatchingModel(\n",
    "    attr_summarizer = dm.attr_summarizers.Hybrid(word_aggregator=lambda: my_word_aggregator_module))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifier\n",
    "\n",
    "This component takes the attribute similarity representations and uses those as features for a classifier that determines whether the input tuple pair refers to the same real-world entity.\n",
    "\n",
    "### Customizing Classifier\n",
    "\n",
    "The ASR can be customized by specifying the `classifier` parameter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 17938410 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(classifier='3-layer-residual-relu')\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `classifier` can be set to one of the following:\n",
    "\n",
    "- A string: A valid `style` string supported by the [`dm.modules.Transform`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Transform) module.\n",
    "- An instance of [`dm.Classifier`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Transform).\n",
    "- A [`callable`](https://docs.python.org/3/library/functions.html#callable): Put simply, a function that returns a PyTorch [Module](http://pytorch.org/docs/master/nn.html#torch.nn.Module). The module must take in one vectors as input and produce the log probability of non-match and match as output. Two outputs are used instead of one to work around a numerical stability issue in torch.\n",
    "  - Input to module: Two 2d tensors of shape `(batch, input_size)`. \n",
    "  - Expected output from module: One 2d tensor of shape `(batch, 2)`. The second dimension must contain non-match and match class probabilities, in that order.\n",
    "\n",
    "`string` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 17757810 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(classifier='2-layer-highway-tanh')\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dm.Classifier` arg example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 19137482 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = dm.MatchingModel(classifier=dm.Classifier(\n",
    "    dm.modules.Transform('3-layer-residual', non_linearity=None, hidden_size=512)))\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`callable` arg example: We create a custom classifier, one that outputs 3 class probabilities (e.g., for text entailment). We also use [`dm.modules.Lambda`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.Lambda) and [`dm.modules.NoMeta`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.NoMeta) as in earlier examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepmatcher.core:Successfully initialized MatchingModel with 17758111 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "my_classifier_module = torch.nn.Sequential(\n",
    "    dm.modules.Transform('2-layer-highway', hidden_size=300),\n",
    "    dm.modules.Transform('1-layer', non_linearity=None, output_size=3),\n",
    "    torch.nn.LogSoftmax(dim=1))\n",
    "\n",
    "model = dm.MatchingModel(classifier=lambda: my_classifier_module)\n",
    "model.initialize(train_dataset)  # Explicitly initialize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Note: Lazy Initialization\n",
    "\n",
    "As mentioned earlier, `deepmatcher` follows a lazy initialization paradigm. This enables it to:\n",
    "1. Easily create clones of modules: These clones share the same structure but have their own separate trainable parameters.\n",
    "2. Automatically infer input sizes: In order to initialize the model `deepmatcher` performs one full forward pass through the model. In this process, each component is initialized sonly after initializing all its parent modules in the computational graph. This makes automatic input size inference for modules possible. As a result, plugging in custom modules in the middle of the network is much easier as you do not have to manually compute the input size.\n",
    "3. Verify module output shapes: Having incorrect output shapes in custom modules can introduce subtle bugs that are difficult to catch. As part of initialization, `deepmatcher` verifies that all modules output tensors with the correct output shapes. This verification is done only once during initialization to avoid slowing down training.\n",
    "\n",
    "It's becase of the above reasons 1 and 2 that `deepmatcher` does not permit custom modules to be specified directly and requires them to be specified via functions.\n",
    "\n",
    "The core module that enables lazy initialization is [`dm.modules.LazyModule`](http://pages.cs.wisc.edu/~sidharth/deepmatcher/modules.html#deepmatcher.models.modules.LazyModule) which is a base class for most modules in `deepmatcher`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
