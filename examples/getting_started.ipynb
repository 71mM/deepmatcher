{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXpIRSAQ-cw1"
   },
   "source": [
    "# Getting started with DeepMatcher\n",
    "\n",
    "Note: you can run **[this notebook live in Google Colab](https://colab.research.google.com/github/sidharthms/deepmatcher/blob/master/examples/getting_started.ipynb)** and use free GPUs provided by Google.\n",
    "\n",
    "This tutorial describes how to effortlessly perform entity matching using deep neural networks. Specifically, we will see how to match pairs of tuples (also called data records or table rows) to determine if they refer to the same real world entity. To do so, we will need labeled examples as input, i.e., tuple pairs which have been annotated as matches or non-matches. This will be used to train our neural network using supervised learning. At the end of this tutorial, you will have a trained neural network as output which you can easily apply to unlabeled tuple pairs to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an overview, here are the 4 steps to use `deepmatcher` which we will go through in this tutorial:\n",
    "\n",
    "<ol start=\"0\">\n",
    "  <li>Setup</li>\n",
    "  <li>Process data</li>\n",
    "  <li>Define neural network model</li>\n",
    "  <li>Train model</li>\n",
    "  <li>Apply model to new data</li>\n",
    "</ol>\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setup\n",
    "\n",
    "If you are running this notebook inside Colab, you will first need to install necessary packages by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LEiDiDNz-PEG"
   },
   "outputs": [],
   "source": [
    "!pip install -q http://download.pytorch.org/whl/cu80/torch-0.3.1-cp36-cp36m-linux_x86_64.whl\n",
    "!pip install -q --process-dependency-links git+https://github.com/sidharthms/deepmatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import `deepmatcher` which will do all the heavy lifting to build and train neural network models for entity matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepmatcher as dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend having a GPU available for the training in Step 4. In case a GPU is not available, we will use all available CPU cores. You can run the following command to determine if a GPU is available and will be used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download sample data for entity matching\n",
    "\n",
    "Now let's get some sample data to play with in this tutorial. We will need three sets of labeled data and one set of unlabeled data:\n",
    "\n",
    "1. **Training Data:** This is used for training our neural network model.\n",
    "2. **Validation Data:** This is used for determining the configuration (i.e., hyperparameters) of our model in such a way that the model does not overfit to the training set.\n",
    "3. **Test Data:** This is used to estimate the performance of our trained model on unlabeled data.\n",
    "4. **Unlabeled Data:** The trained model is applied on this data to obtain predictions, which can then be used for downstream tasks in practical application scenarios.\n",
    "\n",
    "We download these four data sets to the `sample_data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gvPSNrnUuHNF"
   },
   "outputs": [],
   "source": [
    "!mkdir -p sample_data\n",
    "!wget -qnc -P sample_data https://raw.githubusercontent.com/sidharthms/deepmatcher/master/examples/sample_data/amz_goog_train.csv\n",
    "!wget -qnc -P sample_data https://raw.githubusercontent.com/sidharthms/deepmatcher/master/examples/sample_data/amz_goog_validation.csv\n",
    "!wget -qnc -P sample_data https://raw.githubusercontent.com/sidharthms/deepmatcher/master/examples/sample_data/amz_goog_test.csv\n",
    "!wget -qnc -P sample_data https://raw.githubusercontent.com/sidharthms/deepmatcher/master/examples/sample_data/amz_goog_unlabeled.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3034,
     "status": "ok",
     "timestamp": 1523947284733,
     "user": {
      "displayName": "Sidharth Mudgal",
      "photoUrl": "//lh6.googleusercontent.com/-3orlcyRRzwg/AAAAAAAAAAI/AAAAAAAAIWE/SWMfxLxjDVQ/s50-c-k-no/photo.jpg",
      "userId": "102382893000738863249"
     },
     "user_tz": 300
    },
    "id": "aCXeRwfHVM9R",
    "outputId": "fc3e52a2-f1c9-40c9-a28d-0c020b3b02ac"
   },
   "source": [
    "## Step 1. Process data\n",
    "\n",
    "Before we can use our data for training, `deepmatcher` needs to first load and process it in order to prepare it for neural network training. Currently `deepmatcher` only supports processing CSV files. Each CSV file is assumed to have the following kinds of columns:\n",
    "\n",
    "* **\"Left\" attributes (required):** Our goal is to match tuple pairs. \"Left\" attributes are columns that correspond to the \"left\" tuple or the first tuple in the tuple pair. These column names are expected to be prefixed with \"left_\" by default.\n",
    "* **\"Right\" attributes (required):** \"Right\" attributes are columns that correspond to the \"right\" tuple or the second tuple in the tuple pair. These column names are expected to be prefixed with \"right_\" by default.\n",
    "* **Label column (required for train, validation, test):** Column containing the labels (match or non-match) for each tuple pair. Expected to be named \"label\" by default\n",
    "* **ID column (required):** Column containing a unique ID for each tuple pair. This is for evaluation convenience.  Expected to be named \"id\" by default.\n",
    "\n",
    "More details on what data processing involves and ways to customize it are described in **[this notebook](https://github.com/sidharthms/deepmatcher/tree/master/examples/data_process.ipynb)**. \n",
    "\n",
    "### Processing train / validation / test data\n",
    "In order to process our train, validation and test CSV files we call `dm.process` in the following code snippet which will load and process the CSV files and return three processed `MatchingDataset` objects respectively. These dataset objects will later be used for training and evaluation. The basic parameters to `dm.proecss` are as follows:\n",
    "\n",
    "* **path (required): ** The path where all data is stored. This includes train, validation and test. `deepmatcher` may create new files in this directory to store information about these data sets. This allows subsequent `dm.process` calls to be much faster.\n",
    "* **train (required): ** File name of training data in `path` directory.\n",
    "* **validation (required): ** File name of validation data in `path` directory.\n",
    "* **test (optional): ** File name of test data in `path` directory.\n",
    "* **ignore_columns (optional): ** Any columns in the CSV files that you may want to ignore for the purposes of training. These should be included here. \n",
    "\n",
    "Note that the train, validation and test CSVs must all share the same schema, i.e., they should have the same columns. Processing data involves several steps and can take several minutes to complete, especially if this is the first time you are running the `deepmatcher` package.\n",
    "\n",
    "NOTE: If you are running this in Colab, you may get a message saying 'Memory usage is close to the limit.' You can safely ignore it for now. We are working on reducing the memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WJ37wi1HC9EJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load time: 3.7080047791823745\n",
      "Vocab time: 13.488945204764605\n",
      "Metadata time: 1.5047426847741008\n",
      "Cache time: 2.229101464152336\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = dm.process(\n",
    "    path='sample_data',\n",
    "    train='amz_goog_train.csv',\n",
    "    validation='amz_goog_validation.csv',\n",
    "    test='amz_goog_test.csv',\n",
    "    ignore_columns=('left_id', 'right_id'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peeking at processed data\n",
    "Let's take a look at how the processed data looks like. To do this, we get the raw `pandas` table corresponding to the processed training dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = train.get_raw_table()\n",
    "train_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the processed attribute values have been tokenized and lowercased so don't look exactly the same as the input training data. These modifications help the neural network generalize better, i.e., perform better on data not trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing unlabeled data\n",
    "\n",
    "`dm.process` can also be used to process unlabeled data, as shown in the code snippet below, so that you can perform prediction over it. The basic parameters to use `dm.process` for this case are as follows:\n",
    "\n",
    "* **path (required): ** The path where unlabeled data is stored.\n",
    "* **unlabeled (required): ** File name of unlabeled data in `path` directory.\n",
    "* **ignore_columns (optional): ** Any columns in the CSV file that you may want to ignore for the purposes of training. These should be included here.\n",
    "\n",
    "Note that the unlabeled CSV file must have the same schema as the train, validation and test CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = dm.process(\n",
    "    path='sample_data',\n",
    "    unlabeled='amz_goog_unlabeled.csv',\n",
    "    ignore_columns=('left_id', 'right_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Define neural network model\n",
    "\n",
    "In this step you tell `deepmatcher` what kind of neural network you would like to use for entity matching. The easiest way to do this is to use one of the several kinds of neural network models that comes built-in with `deepmatcher`. To use a built-in network, construct a `dm.MatchingModel` as follows:\n",
    "\n",
    "`model = dm.MatchingModel(attr_summarizer='<TYPE>')`\n",
    "\n",
    "where `<TYPE>` is one of `sif`, `rnn`, `attention` or `hybrid`. If you are not familiar with what these mean, we strongly recommend taking a look at either **[slides from our talk on deepmatcher](http://bit.do/deepmatcher-talk)** for a high level overview, or **[our paper](http://pages.cs.wisc.edu/~anhai/papers1/deepmatcher-sigmod18.pdf)** for a more detailed explanation. Here we give briefly describe the intuition behind these four model types:\n",
    "* **sif:** This model considers the **words** present in each attribute value pair to determine a match or non-match. It does not take word order into account.\n",
    "* **rnn:** This model considers the **sequences of words** present in each attribute value pair to determine a match or non-match.\n",
    "* **attention:** This model considers the **alignment of words** present in each attribute value pair to determine a match or non-match. It does not take word order into account.\n",
    "* **hybrid:** This model considers the **alignment of sequences of words** present in each attribute value pair to determine a match or non-match.\n",
    "\n",
    "`deepmatcher` is highly customizable and allows you to tune almost every aspect of the neural network model for your application scenario. **[This tutorial](https://github.com/sidharthms/deepmatcher/tree/master/examples/customize_network.ipynb)** discusses the structure of `MatchingModel`s and how they can be customized.\n",
    "\n",
    "For this tutorial, let's create a `hybrid` model for entity matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dm.MatchingModel(attr_summarizer='hybrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train model\n",
    "\n",
    "Next, we train the defined neural network model using the processed training and validation data. To do so, we call the `run_train` method which takes the following basic parameters:\n",
    "\n",
    "* **train:** The processed training dataset object (of type `MatchingDataset`).\n",
    "* **validation:** The processed validation dataset object (of type `MatchingDataset`).\n",
    "* **epochs:** Number of times to go over the entire `train` data for training the model.\n",
    "* **batch_size:** Number of labeled examples (tuple pairs) to use for each training step. This value may be increased if you have a lot of training data and would like to speed up training. The optimal value is dataset dependent.\n",
    "* **best_save_path:** Path to save the best model.\n",
    "* **pos_weight**: The ratio of the weight of positive examples (matches) to weight of negative examples (non-matches). This value should be increased if you have fewer matches than non-matches in your data. The optimal value is dataset dependent.\n",
    "\n",
    "Many other aspects of the training algorithm can be customized. For details on this, please refer the API documentation for **[run_train]()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_train(\n",
    "    train,\n",
    "    validation,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    best_save_path='hybrid_model.pth',\n",
    "    pos_weight=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Apply model to new data\n",
    "\n",
    "#### Evaluating on test data\n",
    "Now that we have a trained model for entity matching, we can now evaluate its accuracy on test data, to estimate the performance of the model on unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  EVAL Epoch 6 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:    0.3 | Load Time:    0.7 || F1:  58.17 | Prec:  61.03 | Rec:  55.56 || Ex/s: 2228.61\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58.165548098434"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute F1 on test set\n",
    "sif_model.run_eval(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting predictions on unlabeled data\n",
    "\n",
    "We finally apply the trained model to unlabeled data to get predictions. To do this, we call the `run_prediction` method which takes a processed data set object and returns a `pandas` dataframe containing tuple pair IDs (`id` column) and the corresponding match / non-match predictions (`prediction` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.run_prediction(unlabeled)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may optionally set the `output_attributes` parameter to also include all attributes present in the original input table. As mentioned earlier, the processed attribute values will likely look a bit different from the attribute values in the input CSV files due to modifications such as tokenization and lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.run_prediction(unlabeled, output_attributes=True)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then save these predictions to CSV and use them for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('sample_data/unlabeled_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting predictions on labeled data\n",
    "\n",
    "You can also get predictions for labeled data such as validation data. To do so, you can simply call the `run_prediction` method passing the validation data as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = model.run_prediction(validation, output_processed_attributes=True)\n",
    "valid_predictions.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "getting_started.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
